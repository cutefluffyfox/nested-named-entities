{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fc7c642",
   "metadata": {},
   "source": [
    "# Nested Named Entities\n",
    "\n",
    "In this notebook we will dive into idea how to solve Nested Named Entities with use of Bert-like models and **Neural Netwroks**. Here we will go throught the whole process of data preparation, data transformations from raw predictions to human-readable format.\n",
    "\n",
    "NOTE: This solution is not considered to be working in any sort, my macro f1-score is 0.28% (with mean IOU of 38% on eval set). Nevertheless, I am happy with my results and would love to explain basics about NER models and how to make them Nested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec87bc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# this dataset is not required, it was used in the original competition\n",
    "dataset = load_dataset(\"iluvvatar/RuNNE\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cfa3777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# required to translate data to 'iluvvatar/RuNNE' format\n",
    "texts = []\n",
    "entities = []\n",
    "\n",
    "# load train.jsonl\n",
    "with open('../../data/train.jsonl', 'r', encoding='UTF-8') as file:\n",
    "    for line in file.readlines():\n",
    "        cur_json = json.loads(line)\n",
    "        texts.append(cur_json['sentences'])\n",
    "        entities.append([f\"{s} {e} {t}\" for s, e, t in cur_json['ners']])\n",
    "\n",
    "# save to dataframe (you can add dataset.train if you want)\n",
    "train = pd.DataFrame()\n",
    "train['text'] = texts  # + dataset['train'].to_pandas()['text'].to_list()\n",
    "train['entities'] = entities # + dataset['train'].to_pandas()['entities'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d3d8750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Бостон взорвали Тамерлан и Джохар Царнаевы из ...</td>\n",
       "      <td>[0 5 CITY, 16 23 PERSON, 34 41 PERSON, 46 62 L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Умер избитый до комы гитарист и сооснователь г...</td>\n",
       "      <td>[21 28 PROFESSION, 53 67 ORGANIZATION, 100 148...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Путин подписал распоряжение о выходе России из...</td>\n",
       "      <td>[0 4 PERSON, 37 42 COUNTRY, 47 76 ORGANIZATION...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Бенедикт XVI носил кардиостимулятор\\nПапа Римс...</td>\n",
       "      <td>[0 11 PERSON, 36 47 PROFESSION, 49 60 PERSON, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Обама назначит в Верховный суд латиноамериканк...</td>\n",
       "      <td>[0 4 PERSON, 17 29 ORGANIZATION, 48 56 PROFESS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>Глава Малайзии: мы не хотим противостоять Кита...</td>\n",
       "      <td>[42 46 COUNTRY, 82 87 COUNTRY, 104 123 LOCATIO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>«Союз» впервые пристыковался к МКС за 6 часов\\...</td>\n",
       "      <td>[1 4 PRODUCT, 31 33 FACILITY, 35 44 TIME, 48 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>Трамп и Путин сделали совместное заявление к 7...</td>\n",
       "      <td>[0 4 PERSON, 8 12 PERSON, 45 52 AGE, 72 80 PRO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>Российский магнат устроил самую дорогую свадьб...</td>\n",
       "      <td>[0 9 NATIONALITY, 58 72 PERSON, 101 115 PERSON...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>Трамп поздравил астронавта Пегги Уитсон с уста...</td>\n",
       "      <td>[0 4 PERSON, 16 25 PROFESSION, 27 38 PERSON, 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>519 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    Бостон взорвали Тамерлан и Джохар Царнаевы из ...   \n",
       "1    Умер избитый до комы гитарист и сооснователь г...   \n",
       "2    Путин подписал распоряжение о выходе России из...   \n",
       "3    Бенедикт XVI носил кардиостимулятор\\nПапа Римс...   \n",
       "4    Обама назначит в Верховный суд латиноамериканк...   \n",
       "..                                                 ...   \n",
       "514  Глава Малайзии: мы не хотим противостоять Кита...   \n",
       "515  «Союз» впервые пристыковался к МКС за 6 часов\\...   \n",
       "516  Трамп и Путин сделали совместное заявление к 7...   \n",
       "517  Российский магнат устроил самую дорогую свадьб...   \n",
       "518  Трамп поздравил астронавта Пегги Уитсон с уста...   \n",
       "\n",
       "                                              entities  \n",
       "0    [0 5 CITY, 16 23 PERSON, 34 41 PERSON, 46 62 L...  \n",
       "1    [21 28 PROFESSION, 53 67 ORGANIZATION, 100 148...  \n",
       "2    [0 4 PERSON, 37 42 COUNTRY, 47 76 ORGANIZATION...  \n",
       "3    [0 11 PERSON, 36 47 PROFESSION, 49 60 PERSON, ...  \n",
       "4    [0 4 PERSON, 17 29 ORGANIZATION, 48 56 PROFESS...  \n",
       "..                                                 ...  \n",
       "514  [42 46 COUNTRY, 82 87 COUNTRY, 104 123 LOCATIO...  \n",
       "515  [1 4 PRODUCT, 31 33 FACILITY, 35 44 TIME, 48 6...  \n",
       "516  [0 4 PERSON, 8 12 PERSON, 45 52 AGE, 72 80 PRO...  \n",
       "517  [0 9 NATIONALITY, 58 72 PERSON, 101 115 PERSON...  \n",
       "518  [0 4 PERSON, 16 25 PROFESSION, 27 38 PERSON, 8...  \n",
       "\n",
       "[519 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train = dataset['train'].to_pandas().set_index('id')\n",
    "eval_df = dataset['test'].to_pandas().set_index('id')\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dece0449",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "First of all let's discuss data preparation. In target we have slices of data that represent some entity. If we are going to tokenize data, ideally we should make sure that tokenization will not change our boudaries. But it is hard to implement, especially with already existing tokenizers. \n",
    "\n",
    "To overcome this problem, I will suppose that target slices **do not include** subwords. With this assumption we can use `DeepPavlov/rubert-base-cased` tokenizer and many others if we do work tokenization first. However some words would split into two or more, so for consistency we would consider all subword tokens that same entity as original word.\n",
    "\n",
    "This solution is not the best, and some other models could be consideres (such as char-tokenizers). However ideally it should not influence the predictions that much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03efa456",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# determine model & device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bert_checkpoint = 'DeepPavlov/rubert-base-cased'\n",
    "\n",
    "# load all components\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_checkpoint)\n",
    "bert_model = AutoModel.from_pretrained(bert_checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c53682d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "class NestedNerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Pytoch-Dataset for Nested NER \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            raw_df: pd.DataFrame,\n",
    "            tokenizer: AutoTokenizer,\n",
    "            train: bool = True,\n",
    "            label2idx: dict = None\n",
    "\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize dataset via dataframe\n",
    "        \n",
    "        :param raw_df:     pandas dataframe with `text` column\n",
    "        :param tokenizer:  tokenizer to use\n",
    "        :param train:      whether dataset contains `entities` column\n",
    "        :param label2idx:  mapping from label to index\n",
    "        \"\"\"\n",
    "        \n",
    "        # save data descriptors\n",
    "        self.tokenizer = tokenizer  # how to tokenize data\n",
    "        self.train = train  # whether it is train data\n",
    "        self.label2id = dict() if label2idx is None else label2idx  # dict is mutable\n",
    "\n",
    "        # align tokenized text & entities\n",
    "        self.tokenized_texts = []\n",
    "        self.aligned_entities = []\n",
    "\n",
    "        for idx in tqdm(range(raw_df['text'].shape[0])):\n",
    "            # get raw text & entities\n",
    "            ents = raw_df.entities.iloc[idx] if self.train else []\n",
    "            sentence = raw_df.text.iloc[idx]\n",
    "\n",
    "            # word tokenization\n",
    "            new_sent, new_ents = self.get_word_tokens_ents(sentence, ents)\n",
    "\n",
    "            # usual tokenization\n",
    "            inputs = self.tokenizer(\n",
    "                new_sent,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                is_split_into_words=True\n",
    "            )\n",
    "\n",
    "            # align labels with tokens\n",
    "            aligned_ents = []\n",
    "            if self.train:\n",
    "                aligned_ents = self.align_labels_with_tokens(new_ents, inputs.word_ids())\n",
    "\n",
    "            # save preprocessed data\n",
    "            self.tokenized_texts.append(inputs)\n",
    "            self.aligned_entities.append(aligned_ents)\n",
    "\n",
    "        # compute length of dataset\n",
    "        self.length = len(self.aligned_entities)\n",
    "\n",
    "    def get_word_tokens_ents(self, sentence: str, ents: list[str]) -> tuple[list[str], list[list[str]]]:\n",
    "        \"\"\"\n",
    "        Word tokenization for sentence with alignment of entities\n",
    "        \n",
    "        :param sentence:  sentence to split to words\n",
    "        :param ents:      raw entities in given sentence\n",
    "        :return:          (word tokenized sentence, aligned entities)\n",
    "        \"\"\"\n",
    "        \n",
    "        # ent index to post-tokenized index\n",
    "        idx_to_post_process_idx = dict()\n",
    "\n",
    "        # new word tokens (basically split by ' \\t\\n')\n",
    "        word_tokens = ['']\n",
    "        len_tokens = 0\n",
    "        for idx, symb in enumerate(sentence):\n",
    "            if symb in {' ', '\\t', '\\n'}:\n",
    "\n",
    "                # if new word is not already created\n",
    "                if word_tokens and word_tokens[-1] != '':\n",
    "                    word_tokens.append('')\n",
    "                    len_tokens += 1\n",
    "                continue\n",
    "\n",
    "            # add symbol to last word\n",
    "            word_tokens[-1] += symb\n",
    "            # map symbol index to new token index\n",
    "            idx_to_post_process_idx[idx] = len_tokens\n",
    "\n",
    "        # remove empty symbol from the back (if needed)\n",
    "        if word_tokens[-1] == '':\n",
    "            word_tokens.pop()\n",
    "\n",
    "        # new entetis\n",
    "        new_ents = [[] for _ in range(len(word_tokens))]\n",
    "\n",
    "        for ent in ents:\n",
    "            # parse raw ents\n",
    "            start_idx, end_idx, ent_name = ent.split()\n",
    "            start_idx, end_idx = int(start_idx), int(end_idx)\n",
    "            ent_name = self.label2id.get(ent_name, ent_name)\n",
    "\n",
    "            # control number of adds\n",
    "            placed_idxs = dict()\n",
    "\n",
    "            for idx in range(start_idx, end_idx):\n",
    "                # transform symb index to token index\n",
    "                new_idx = idx_to_post_process_idx.get(idx)\n",
    "\n",
    "                # if present & not already placed, place it\n",
    "                if (new_idx is not None) and (not placed_idxs.get(new_idx, False)):\n",
    "                    new_ents[new_idx].append(ent_name)\n",
    "                    placed_idxs[new_idx] = True\n",
    "\n",
    "        return word_tokens, new_ents\n",
    "\n",
    "    @staticmethod\n",
    "    def align_labels_with_tokens(labels: list[list[str]], word_ids: list[int]) -> list[list[str]]:\n",
    "        \"\"\"\n",
    "        Align entities labels with post-tokenized words\n",
    "        \n",
    "        :param labels:    entities from word tokenization\n",
    "        :param word_ids:  word id's from `self.tokenizer`\n",
    "        :return:          entities aligned to `word_ids` \n",
    "        \"\"\"\n",
    "        \n",
    "        # aligned labels\n",
    "        new_labels = []\n",
    "        current_word = None\n",
    "\n",
    "        for word_id in word_ids:\n",
    "            if word_id != current_word:\n",
    "                # start of a new word => add new label\n",
    "                current_word = word_id\n",
    "                label = [] if word_id is None else labels[word_id]\n",
    "                new_labels.append(label)\n",
    "            elif word_id is None:\n",
    "                # Special token => add empty label\n",
    "                new_labels.append([])\n",
    "            else:\n",
    "                # Same word as previous token => copy label\n",
    "                label = labels[word_id]\n",
    "                new_labels.append(label)\n",
    "\n",
    "        return new_labels\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.tokenized_texts[idx], self.aligned_entities[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f421b632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 519/519 [00:02<00:00, 239.39it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 93/93 [00:00<00:00, 166.36it/s]\n"
     ]
    }
   ],
   "source": [
    "def extract_ent_labels(raw_df: pd.DataFrame) -> set[str]:\n",
    "    \"\"\"\n",
    "    Extract entities names from dataframe\n",
    "    \n",
    "    :param raw_df:  dataframe with `entities` column\n",
    "    :return:        set of entities names\n",
    "    \"\"\"\n",
    "    \n",
    "    ent_names = set()\n",
    "    for ents in raw_df['entities']:\n",
    "        for ent in ents:\n",
    "            ent_names.add(ent.split()[-1])\n",
    "    return ent_names\n",
    "\n",
    "\n",
    "# entities encoding/decoding\n",
    "label2idx = dict()\n",
    "idx2label = dict()\n",
    "\n",
    "# for each entity make an index\n",
    "for idx, label in enumerate(sorted(extract_ent_labels(train))):\n",
    "    label2idx[label] = idx\n",
    "    idx2label[idx] = label\n",
    "\n",
    "\n",
    "# define train / eval dataloaders\n",
    "train_dataloader = NestedNerDataset(train.copy(), tokenizer=tokenizer, label2idx=label2idx)\n",
    "eval_dataloader = NestedNerDataset(eval_df.copy(), tokenizer=tokenizer, label2idx=label2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cb9e25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 299])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = train_dataloader[0]\n",
    "sample[0]['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b47358",
   "metadata": {},
   "source": [
    "### Model architecture\n",
    "\n",
    "Now when we have data preprocessed let's discuss the architecture.\n",
    "\n",
    "It is obvious that we will use bert (as we already preprocessed data for them), but how? The idea I want to implement is - get token embiddings with bert-like models, and then train classifiers for token classification. As we have 29 classes of data, and we want \"nested\" dependencies, we will train 29 binary classifiers (one for each entity type). \n",
    "\n",
    "This architecture is simmilar to one developed by [SinaLab](https://github.com/SinaLab/ArabicNER) for Arabic. \n",
    "\n",
    "The main difference is that they had several classes and each class had different types (Like entity \"Human\" could be \"Name\", \"Surname\", \"Age\" e.t.c). So my solution is simplification of their architecture. However my solution could be simply rewritten if required. \n",
    "\n",
    "Also they trained their own bert model, however I am interested how raw bert (not even fine-tuned) would perform in such task. So in implementation below I freeze all bert-layers. This boosts training process of the model from 15 minutes per epoch, to just 30 seconds.\n",
    "\n",
    "Now let's discuss classifiers a bit. In SinaLab they used two-layer NN (786->hid->1), but I found out that multi-layers with dropout works better. So you can adjust number of layers (at least 2) and dropout rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5a4f416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Binary classifier for word embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_dim: int,\n",
    "            hidden_dim: int,\n",
    "            n_layers: int = 2,\n",
    "            dropout: float = 0.1,\n",
    "            *args,\n",
    "            **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Define classifier by key-parameters\n",
    "\n",
    "        :param in_dim:      embedding dimension\n",
    "        :param hidden_dim:  dimension of hidden layers\n",
    "        :param n_layers:    number of hidden layers (at least 2)\n",
    "        :param dropout:     dropout probability\n",
    "        :param args:        additional arguments\n",
    "        :param kwargs:      additional keyword arguments\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        # define linear & dropout layers\n",
    "        self.n_layers = n_layers\n",
    "        linear_layers = [nn.Linear(in_dim, hidden_dim)]  # from in_dim\n",
    "        dropout_layers = [nn.Dropout(dropout)]\n",
    "\n",
    "        for i in range(n_layers - 2):\n",
    "            linear_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            dropout_layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        linear_layers.append(nn.Linear(hidden_dim, 1))  # to prediction\n",
    "\n",
    "        # save layers as sequential models (for gradient flow)\n",
    "        self.linear_layers = torch.nn.Sequential(*linear_layers)\n",
    "        self.dropouts = torch.nn.Sequential(*dropout_layers)\n",
    "\n",
    "        # activation function & output\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.linear_layers) - 1):\n",
    "            # dropout -> linear -> relu\n",
    "            x = self.dropouts[i](x)\n",
    "            x = self.relu(self.linear_layers[i](x))\n",
    "\n",
    "        # linear -> sigmoid\n",
    "        x = self.sigmoid(self.linear_layers[-1](x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class BertNestedTagger(nn.Module):\n",
    "    \"\"\"\n",
    "    Bert-Based Nested Tagger\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            bert_model: BertModel,\n",
    "            device,\n",
    "            dropout: float = 0.1,\n",
    "            n_classes: int = 2,\n",
    "            n_layers: int = 4,\n",
    "            hid_dim: int = 512,\n",
    "            *args,\n",
    "            **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Define model by key parameters\n",
    "\n",
    "        :param bert_model:  instance of bert-like model\n",
    "        :param device:      device to run the model on\n",
    "        :param dropout:     dropout probability (for classifiers)\n",
    "        :param n_classes:   number of output classes (number of classifiers)\n",
    "        :param n_layers:    number of hidden layers (for classifiers)\n",
    "        :param hid_dim:     dimension of hidden layers (for classifiers)\n",
    "        :param args:        additional arguments\n",
    "        :param kwargs:      additional keyword arguments\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        # save bert model\n",
    "        self.bert_embeddings = bert_model\n",
    "\n",
    "        # define and save all classifiers\n",
    "        self.n_classes = n_classes\n",
    "        classifiers = [\n",
    "            Classifier(in_dim=768, hidden_dim=hid_dim, n_layers=n_layers, dropout=dropout).to(device)\n",
    "            for _ in range(self.n_classes)\n",
    "        ]\n",
    "        self.classifiers = torch.nn.Sequential(*classifiers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # y [batch_size x max_batch_token_len x 786]\n",
    "        y = self.bert_embeddings(**x)['last_hidden_state']\n",
    "        output = list()\n",
    "\n",
    "        for i, classifier in enumerate(self.classifiers):\n",
    "            # logits [batch_size x max_batch_token_len x 1]\n",
    "            logits = classifier(y)\n",
    "            output.append(logits)\n",
    "\n",
    "        # [batch_size x max_batch_token_len x n_classes x 1]\n",
    "        output = torch.stack(output).permute((1, 2, 0, 3))\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3341d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader: NestedNerDataset, model: BertNestedTagger, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Train model for 1 epoch\n",
    "\n",
    "    :param dataloader:  NestedNerDataset class instance\n",
    "    :param model:       BertNestedTagger class instance\n",
    "    :param optimizer:   pytorch optimizer\n",
    "    :param criterion:   loss function\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in tqdm(dataloader, total=len(dataloader)):\n",
    "        # get labels & tokens\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # get predictions (for each token 29 classes)\n",
    "        # as batch_size is always 1, we can squeeze\n",
    "        # preds [n_tokens, 29]\n",
    "        preds = model(inputs).squeeze().to('cpu')\n",
    "\n",
    "        # flatten for simpler loss calculations \n",
    "        # preds [n_tokens * 29 x 1]\n",
    "        preds = preds.flatten()\n",
    "\n",
    "        # transform targets\n",
    "        transformed_labels = []\n",
    "        for ind in range(len(labels)):\n",
    "            label_ids = labels[ind]\n",
    "            # for each class index check whether it's in the labels\n",
    "            transformed_labels.extend([float(class_idx in labels[ind]) for class_idx in range(model.n_classes)])\n",
    "\n",
    "        # transform to torch.tensor and calculate loss\n",
    "        torch_labels = torch.tensor(transformed_labels).to('cpu')\n",
    "        loss = criterion(preds, torch_labels)\n",
    "\n",
    "        # update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def get_loss_epoch(dataloader: NestedNerDataset, model: BertNestedTagger, criterion):\n",
    "    \"\"\"\n",
    "    Get loss for 1 epoch\n",
    "    \n",
    "    :param dataloader:  NestedNerDataset class instance\n",
    "    :param model:       BertNestedTagger class instance\n",
    "    :param criterion:   loss function\n",
    "    :return:            average loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in tqdm(dataloader, total=len(dataloader)):\n",
    "        # get labels & tokens\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        # preds [n_tokens, 29]\n",
    "        preds = model(inputs).squeeze().to('cpu')\n",
    "        preds = preds.flatten()\n",
    "\n",
    "        # transform targets\n",
    "        transformed_labels = []\n",
    "        for ind in range(len(labels)):\n",
    "            label_ids = labels[ind]\n",
    "            # for each class index check whether it's in the labels\n",
    "            transformed_labels.extend([float(class_idx in labels[ind]) for class_idx in range(model.n_classes)])\n",
    "\n",
    "        # transform to torch.tensor and calculate loss\n",
    "        torch_labels = torch.tensor(transformed_labels).to('cpu')\n",
    "        loss = criterion(preds, torch_labels)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ddd320",
   "metadata": {},
   "source": [
    "### From Predictions -> Human-Readable\n",
    "\n",
    "And finally when we have predictions, it make sence to discuss how wo transform raw token prediction back to slices & labels format. \n",
    "\n",
    "This was one of the most confusing steps for me, as we are not working with full words but subwords tokenization. \n",
    "\n",
    "The first thing is data we are getting is just a list of lists where each `ans[i]` correspond to `i`'th token, and `ans[i]` contains indexes of classes given token corresponds to. With this information we could potentially get tokens start/end indexes in original sentence. This is what is implemented in `map_results_to_ids` function. Basically it takes `token_id`, via tokenizer get the string representation and finds string map in the original sentence. This implementation is pretty nice as it do not cares about word tokenization (we don't need to consider skipped spaces/tabs/new-lines e.t.c.). Algorithm is modified a bit for `DeepPavlov/rubert-base-cased` as it strips special tokens that start with `#`. I tested this method on train/eval data and it resulted in perfect map. \n",
    "\n",
    "The second step is to combine continiouts entities. As an input we have (start, end, entities_list) and we want to combine tokens that we consider `continious` (e.g. `end[i] == start[i]`). For this we can use scanline algorithm implemented in `from_sequential_to_readable` function. Worth mentioning that we can define `error` of combination, where tokens on disstance less than `error` could be considered as one, this adds variability to result where results and could boost performance if error is tuned as needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53094f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(texts: list[str], model: BertNestedTagger, idx2label: dict[int, str], print_res: bool = False):\n",
    "    \"\"\"\n",
    "    Get model predictions for given texts\n",
    "\n",
    "    :param texts:      texts for inference\n",
    "    :param model:      BertNestedTagger class instance\n",
    "    :param idx2label:  mapping from index to entity label\n",
    "    :param print_res:  whether to print predictions to console\n",
    "    :return:           model predictions\n",
    "    \"\"\"\n",
    "\n",
    "    # transform list data to NestedNerDataset\n",
    "    inf_df = pd.DataFrame()\n",
    "    inf_df['text'] = texts\n",
    "    inference_loader = NestedNerDataset(\n",
    "        inf_df,\n",
    "        tokenizer=tokenizer,\n",
    "        label2idx=label2idx,\n",
    "        train=False)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # get inference results\n",
    "    results = []\n",
    "    for data in tqdm(inference_loader, total=len(inference_loader)):\n",
    "        results.append([])\n",
    "\n",
    "        inputs, _ = data\n",
    "        inputs = inputs.to(model.device)\n",
    "\n",
    "        # preds [n_tokens, 29]\n",
    "        preds = model(inputs).squeeze().to('cpu')\n",
    "\n",
    "        # for each token find out what indexes have > 0.5 threshold & save them\n",
    "        for ind in range(preds.shape[0]):\n",
    "            # determine indexes of positive classes\n",
    "            ners = (preds[ind] > 0.5).nonzero(as_tuple=True)[0].tolist()\n",
    "\n",
    "            # transform indexes to entities labels\n",
    "            ner_tags = [idx2label[ner_idx] for ner_idx in ners]\n",
    "\n",
    "            # get token string representation\n",
    "            token_str = model.tokenizer.decode(inputs['input_ids'][0][ind])\n",
    "\n",
    "            if print_res:\n",
    "                print(token_str, '\\t:\\t', ' + '.join(ner_tags))\n",
    "\n",
    "            # save token_id, token_str and ner labels\n",
    "            results[-1].append((\n",
    "                inputs['input_ids'][0][ind].item(),\n",
    "                token_str,\n",
    "                ner_tags\n",
    "            ))\n",
    "\n",
    "        if print_res:\n",
    "            print('=' * 50)\n",
    "    return results\n",
    "\n",
    "\n",
    "def map_results_to_ids(origs: list[str], results: list[list[tuple[int, str, list[str]]]]) -> list[(int, int, list[str])]:\n",
    "    \"\"\"\n",
    "    Get tokens start_idx, end_idx from the original text\n",
    "\n",
    "    :param origs:    original pre-tokenized texts\n",
    "    :param results:  raw predictions of model\n",
    "    :return:         tuples with indexes of tokens in original text\n",
    "    \"\"\"\n",
    "\n",
    "    answers = []\n",
    "    for orig, preds in zip(origs, results):\n",
    "        orig_ind = 0\n",
    "        ans = []\n",
    "\n",
    "        # remove <BOS> <EOS>\n",
    "        for token_id, str_token, labels in preds[1:-1]:\n",
    "            if str_token != '#':  # DeepPavlov/rubert-base-cased specifics\n",
    "                str_token = str_token.lstrip('#')\n",
    "            \n",
    "            # find start index of str_token in original text\n",
    "            find_idx = orig[orig_ind:].find(str_token)\n",
    "            \n",
    "            # (if token was <UNK>)\n",
    "            if find_idx == -1:\n",
    "                continue\n",
    "            \n",
    "            # update indexes \n",
    "            start_ind = orig_ind + find_idx\n",
    "            end_ind = start_ind + len(str_token)\n",
    "            orig_ind = end_ind\n",
    "            \n",
    "            # always true, but just in case smth is wrong\n",
    "            if str_token == orig[start_ind:end_ind]:  \n",
    "                ans.append((start_ind, end_ind, labels))\n",
    "            \n",
    "        answers.append(ans)\n",
    "    return answers\n",
    "\n",
    "\n",
    "def from_sequential_to_readable(\n",
    "        sequential_results: list[list[int, int, list[int]]],\n",
    "        error: int = 0\n",
    ") -> list[tuple[int, int, str]]:\n",
    "    \"\"\"\n",
    "    Combine continuous tokens into one with some error\n",
    "\n",
    "    :param sequential_results:  outputs of `map_results_to_ids`\n",
    "    :param error:               extra  between tokens to consider 'same'\n",
    "    :return:                    combined tokens\n",
    "    \"\"\"\n",
    "\n",
    "    answers = []\n",
    "    for sequence in sequential_results:\n",
    "        points = []\n",
    "        possible_labels = set()\n",
    "        \n",
    "        # split several labels to several data points\n",
    "        # example (0, 10, [PERSON, DATE] -> [(0, 10, PERSON), (0, 10, DATE)])\n",
    "        for start_idx, end_idx, labels in sequence:\n",
    "            for label in labels:\n",
    "                points.append((start_idx, end_idx, label))\n",
    "                possible_labels.add(label)\n",
    "        \n",
    "        # to remove data cleaning further\n",
    "        for label in possible_labels:\n",
    "            points.append((int(1e9), int(1e9), label))\n",
    "        \n",
    "        ans = []\n",
    "        prev_encounters = dict()\n",
    "        # point already sorted, so now we can implement scanline\n",
    "        for start_idx, end_idx, label in points:\n",
    "            # if too far from previous token, make new one\n",
    "            if prev_encounters.get(label, [-int(1e9), -int(1e9)])[1] + error < start_idx:\n",
    "                # if not first, add to answer\n",
    "                if prev_encounters.get(label) is not None:\n",
    "                    ans.append([*prev_encounters[label], label])\n",
    "                \n",
    "                prev_encounters[label] = [start_idx, end_idx]\n",
    "            # if not too far, change end_idx\n",
    "            else:  \n",
    "                prev_encounters[label][1] = end_idx\n",
    "        answers.append(tuple(ans))\n",
    "\n",
    "    return answers\n",
    "\n",
    "\n",
    "def get_answers(texts: list[str], model: BertNestedTagger, idx2label: dict[int, str]) -> list[tuple[int, int, str]]:\n",
    "    \"\"\"\n",
    "    Full pipeline that runs `map_results_to_ids` -> `mapped` -> `from_sequential_to_readable` sequentially\n",
    "\n",
    "    :param texts:      texts for inference\n",
    "    :param model:      BertNestedTagger class instance\n",
    "    :param idx2label:  mapping from index to entity label\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    raw_predictions = inference(texts, model, idx2label, print_res=False)\n",
    "    mapped = map_results_to_ids(texts, raw_predictions)\n",
    "    sequential = from_sequential_to_readable(mapped, error=2)\n",
    "    return sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d30e13f",
   "metadata": {},
   "source": [
    "### Metric calculation \n",
    "\n",
    "Even though model will be measured by f1-macro, I wanted to see how model performs in general (something like accuracy of some sort). As we are working with sets (both ranges and class type), the simplest thing would be to consider set's IOU. This metric is not interpretable and do not shows insides where model makes mistakes, but it is nice to track how model performs \"in general\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5aa674c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_iou(texts, labels, model, idx2label):\n",
    "    ious = []\n",
    "    for model_prediction, target_label in zip(get_answers(texts, model, idx2label), labels):\n",
    "        # transform to original format\n",
    "        model_prediction = [f\"{s} {e} {t}\" for s, e, t in model_prediction]\n",
    "        \n",
    "        iou = round(100*len(set(model_prediction) & set(target_label)) / len(set(model_prediction) | set(target_label)), 3)\n",
    "        ious.append(iou)\n",
    "    return np.array(ious)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a7fc71",
   "metadata": {},
   "source": [
    "### Train, Evaluate and Inference\n",
    "\n",
    "Basically combination of everything above to test solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7f0207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, eval_dataloader, eval_df, model, n_epochs, learning_rate=3e-5):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        loss_train = train_epoch(train_dataloader, model, optimizer, criterion)\n",
    "        loss_eval = get_loss_epoch(eval_dataloader, model, criterion)\n",
    "        \n",
    "        print(\"Epoch:\", epoch, \"loss:\", loss_train, 'eval_loss:', loss_eval)\n",
    "        ious = get_iou(eval_df.text, eval_df.entities, model, idx2label)\n",
    "        print('Mean IOU:', ious.mean(), 'IOU std:', ious.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c35ff17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertNestedTagger(\n",
       "  (bert_embeddings): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifiers): Sequential(\n",
       "    (0): Classifier(\n",
       "      (linear_layers): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (dropouts): Sequential(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (1): Classifier(\n",
       "      (linear_layers): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (dropouts): Sequential(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (2): Classifier(\n",
       "      (linear_layers): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (dropouts): Sequential(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (3): Classifier(\n",
       "      (linear_layers): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (dropouts): Sequential(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (4): Classifier(\n",
       "      (linear_layers): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (dropouts): Sequential(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (5): Classifier(\n",
       "      (linear_layers): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (dropouts): Sequential(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (6): Classifier(\n",
       "      (linear_layers): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (dropouts): Sequential(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (7): Classifier(\n",
       "      (linear_layers): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (dropouts): Sequential(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (8): Classifier(\n",
       "      (linear_layers): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (dropouts): Sequential(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (9): Classifier(\n",
       "      (linear_layers): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (dropouts): Sequential(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (10): Classifier(\n",
       "      (linear_layers): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (dropouts): Sequential(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (11): Classifier(\n",
       "      (linear_layers): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (dropouts): Sequential(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (12): Classifier(\n",
       "      (linear_layers): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (dropouts): Sequential(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (13): Classifier(\n",
       "      (linear_layers): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (dropouts): Sequential(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (14): Classifier(\n",
       "      (linear_layers): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (dropouts): Sequential(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (15): Classifier(\n",
       "      (linear_layers): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (dropouts): Sequential(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (16): Classifier(\n",
       "      (linear_layers): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (dropouts): Sequential(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (17): Classifier(\n",
       "      (linear_layers): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (dropouts): Sequential(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (18): Classifier(\n",
       "      (linear_layers): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (dropouts): Sequential(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (19): Classifier(\n",
       "      (linear_layers): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (dropouts): Sequential(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (20): Classifier(\n",
       "      (linear_layers): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (dropouts): Sequential(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (21): Classifier(\n",
       "      (linear_layers): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (dropouts): Sequential(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (22): Classifier(\n",
       "      (linear_layers): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (dropouts): Sequential(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (23): Classifier(\n",
       "      (linear_layers): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (dropouts): Sequential(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (24): Classifier(\n",
       "      (linear_layers): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (dropouts): Sequential(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (25): Classifier(\n",
       "      (linear_layers): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (dropouts): Sequential(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (26): Classifier(\n",
       "      (linear_layers): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (dropouts): Sequential(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (27): Classifier(\n",
       "      (linear_layers): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (dropouts): Sequential(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (28): Classifier(\n",
       "      (linear_layers): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (dropouts): Sequential(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertNestedTagger(\n",
    "    bert_model=bert_model, \n",
    "    device=device, \n",
    "    n_classes=len(label2idx), \n",
    "    dropout=0.1,\n",
    "    hid_dim=256,\n",
    "    n_layers=4\n",
    ").to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1954c432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze bert layers\n",
    "for name, param in model.named_parameters():\n",
    "    if 'bert_embeddings' in name:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a604d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 519/519 [00:27<00:00, 19.17it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 93/93 [00:02<00:00, 34.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 loss: 0.02223345344914895 eval_loss: 0.024261629519363243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 93/93 [00:00<00:00, 325.23it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 93/93 [00:08<00:00, 10.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean IOU: 37.7764623655914 IOU std: 9.091316663646205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 519/519 [00:28<00:00, 18.42it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 93/93 [00:02<00:00, 33.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 0.021580167082434445 eval_loss: 0.024081445449302272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 93/93 [00:00<00:00, 325.17it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 93/93 [00:08<00:00, 11.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean IOU: 37.653021505376344 IOU std: 9.285391946005046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 0.0249 : 4x256 [11, 3e-5] + dropout=0.1\n",
    "# 0.0246 : 10x512 [7, 3e-5] + dropout=0.1\n",
    "# 0.0257 : 4x256 [14, 3e-5]\n",
    "# 0.0258 : 4x256 [6, 1e-4]\n",
    "# 0.0258 : 4x256 [13, 1e-5]\n",
    "# 0.0262 : 4x126 [9, 1e-5]\n",
    "# 0.0265 : 5x512 [4, 1e-4]\n",
    "# 0.0267 : 4x128 [3, 1e-3]\n",
    "train(train_dataloader, eval_dataloader, eval_df, model, n_epochs=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd6b42e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 499.44it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 503.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.63it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 333.17it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.87it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 251.85it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.85it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 247.90it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.13it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 198.91it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.33it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 199.62it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.15it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 202.88it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.30it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 199.99it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.54it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 333.33it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 16.94it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 333.30it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.15it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 499.44it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 19.59it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 247.96it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.00it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 142.23it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.81it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.17it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 143.55it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.87it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 500.39it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17.54it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 333.44it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 18.52it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 333.09it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.05it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 335.92it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17.55it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 15.87it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 333.33it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.33it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 166.62it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.93it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 333.38it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17.24it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 333.25it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.49it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 251.01it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.31it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 249.97it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.34it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 337.14it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.98it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 331.88it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.51it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 335.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.09it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 166.64it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.81it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 335.06it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 15.39it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 249.99it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.99it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 499.80it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 16.94it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 336.86it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.91it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 249.94it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.65it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 329.97it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.08it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 249.69it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.41it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 333.38it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.16it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 332.80it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.33it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 333.38it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.29it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 249.97it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.01it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 497.31it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17.23it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 250.05it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.49it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 333.01it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.28it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 331.59it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.92it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 199.24it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.17it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 200.84it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.01it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 333.30it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 15.38it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 500.27it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 16.39it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 100.00it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.52it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 200.00it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.90it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 332.75it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.99it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 333.33it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.16it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 166.51it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.29it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 250.00it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.06it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 332.51it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.66it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 500.27it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.66it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 250.08it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.52it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 166.53it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.79it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 331.02it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.82it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 252.00it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.63it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 199.92it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.58it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 125.01it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.14it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 124.99it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.25it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# make predictions on test data\n",
    "input_jsons = []\n",
    "with open('../../data/test.jsonl', 'r', encoding='UTF-8') as file:\n",
    "    for line in file.readlines():\n",
    "        input_jsons.append(json.loads(line))\n",
    "\n",
    "with open('test.jsonl', 'w') as file:\n",
    "    for test_json in input_jsons:\n",
    "        ans_json = {\n",
    "            'id': test_json['id'],\n",
    "            'ners': get_answers([test_json['senences']], model, idx2label)[0]\n",
    "        }\n",
    "        \n",
    "        file.write(json.dumps(ans_json) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0260190",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
