{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "509f8c56",
   "metadata": {},
   "source": [
    "# Nested Named Entities\n",
    "\n",
    "In this notebook we will dive into idea how to solve Nested Named Entities with use of Bert-like models and **Catboost**. Here we will go throught the whole process of data preparation, network definition and data transformations from raw predictions to human-readable format.\n",
    "\n",
    "NOTE: This solution is not considered to be working in any sort, they takes too much time to inference even 1 sentence. Nevertheless, I am happy with my results and would love to explain basics about NER models token classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3275b7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# this dataset is not required, it was used in the original competition\n",
    "dataset = load_dataset(\"iluvvatar/RuNNE\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7f3f9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# required to translate data to 'iluvvatar/RuNNE' format\n",
    "texts = []\n",
    "entities = []\n",
    "\n",
    "# load train.jsonl\n",
    "with open('../../data/train.jsonl', 'r', encoding='UTF-8') as file:\n",
    "    for line in file.readlines():\n",
    "        cur_json = json.loads(line)\n",
    "        texts.append(cur_json['sentences'])\n",
    "        entities.append([f\"{s} {e} {t}\" for s, e, t in cur_json['ners']])\n",
    "\n",
    "# save to dataframe (you can add dataset.train if you want)\n",
    "train = pd.DataFrame()\n",
    "train['text'] = texts  # + dataset['train'].to_pandas()['text'].to_list()\n",
    "train['entities'] = entities # + dataset['train'].to_pandas()['entities'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75194f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Бостон взорвали Тамерлан и Джохар Царнаевы из ...</td>\n",
       "      <td>[0 5 CITY, 16 23 PERSON, 34 41 PERSON, 46 62 L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Умер избитый до комы гитарист и сооснователь г...</td>\n",
       "      <td>[21 28 PROFESSION, 53 67 ORGANIZATION, 100 148...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Путин подписал распоряжение о выходе России из...</td>\n",
       "      <td>[0 4 PERSON, 37 42 COUNTRY, 47 76 ORGANIZATION...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Бенедикт XVI носил кардиостимулятор\\nПапа Римс...</td>\n",
       "      <td>[0 11 PERSON, 36 47 PROFESSION, 49 60 PERSON, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Обама назначит в Верховный суд латиноамериканк...</td>\n",
       "      <td>[0 4 PERSON, 17 29 ORGANIZATION, 48 56 PROFESS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>Глава Малайзии: мы не хотим противостоять Кита...</td>\n",
       "      <td>[42 46 COUNTRY, 82 87 COUNTRY, 104 123 LOCATIO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>«Союз» впервые пристыковался к МКС за 6 часов\\...</td>\n",
       "      <td>[1 4 PRODUCT, 31 33 FACILITY, 35 44 TIME, 48 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>Трамп и Путин сделали совместное заявление к 7...</td>\n",
       "      <td>[0 4 PERSON, 8 12 PERSON, 45 52 AGE, 72 80 PRO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>Российский магнат устроил самую дорогую свадьб...</td>\n",
       "      <td>[0 9 NATIONALITY, 58 72 PERSON, 101 115 PERSON...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>Трамп поздравил астронавта Пегги Уитсон с уста...</td>\n",
       "      <td>[0 4 PERSON, 16 25 PROFESSION, 27 38 PERSON, 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>519 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    Бостон взорвали Тамерлан и Джохар Царнаевы из ...   \n",
       "1    Умер избитый до комы гитарист и сооснователь г...   \n",
       "2    Путин подписал распоряжение о выходе России из...   \n",
       "3    Бенедикт XVI носил кардиостимулятор\\nПапа Римс...   \n",
       "4    Обама назначит в Верховный суд латиноамериканк...   \n",
       "..                                                 ...   \n",
       "514  Глава Малайзии: мы не хотим противостоять Кита...   \n",
       "515  «Союз» впервые пристыковался к МКС за 6 часов\\...   \n",
       "516  Трамп и Путин сделали совместное заявление к 7...   \n",
       "517  Российский магнат устроил самую дорогую свадьб...   \n",
       "518  Трамп поздравил астронавта Пегги Уитсон с уста...   \n",
       "\n",
       "                                              entities  \n",
       "0    [0 5 CITY, 16 23 PERSON, 34 41 PERSON, 46 62 L...  \n",
       "1    [21 28 PROFESSION, 53 67 ORGANIZATION, 100 148...  \n",
       "2    [0 4 PERSON, 37 42 COUNTRY, 47 76 ORGANIZATION...  \n",
       "3    [0 11 PERSON, 36 47 PROFESSION, 49 60 PERSON, ...  \n",
       "4    [0 4 PERSON, 17 29 ORGANIZATION, 48 56 PROFESS...  \n",
       "..                                                 ...  \n",
       "514  [42 46 COUNTRY, 82 87 COUNTRY, 104 123 LOCATIO...  \n",
       "515  [1 4 PRODUCT, 31 33 FACILITY, 35 44 TIME, 48 6...  \n",
       "516  [0 4 PERSON, 8 12 PERSON, 45 52 AGE, 72 80 PRO...  \n",
       "517  [0 9 NATIONALITY, 58 72 PERSON, 101 115 PERSON...  \n",
       "518  [0 4 PERSON, 16 25 PROFESSION, 27 38 PERSON, 8...  \n",
       "\n",
       "[519 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train = dataset['train'].to_pandas().set_index('id')\n",
    "eval_df = dataset['test'].to_pandas().set_index('id')\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e90c7a",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "First of all let's discuss data preparation. In target we have slices of data that represent some entity. If we are going to tokenize data, ideally we should make sure that tokenization will not change our boudaries. But it is hard to implement, especially with already existing tokenizers. \n",
    "\n",
    "To overcome this problem, I will suppose that target slices **do not include** subwords. With this assumption we can use `DeepPavlov/rubert-base-cased` tokenizer and many others if we do work tokenization first. However some words would split into two or more, so for consistency we would consider all subword tokens that same entity as original word.\n",
    "\n",
    "This solution is not the best, and some other models could be consideres (such as char-tokenizers). However ideally it should not influence the predictions that much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4ed355d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bert_checkpoint = 'DeepPavlov/rubert-base-cased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_checkpoint)\n",
    "bert_model = AutoModel.from_pretrained(bert_checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72ebbe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "class NestedNerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Pytoch-Dataset for Nested NER \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            raw_df: pd.DataFrame,\n",
    "            tokenizer: AutoTokenizer,\n",
    "            train: bool = True,\n",
    "            label2idx: dict = None\n",
    "\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize dataset via dataframe\n",
    "        \n",
    "        :param raw_df:     pandas dataframe with `text` column\n",
    "        :param tokenizer:  tokenizer to use\n",
    "        :param train:      whether dataset contains `entities` column\n",
    "        :param label2idx:  mapping from label to index\n",
    "        \"\"\"\n",
    "        \n",
    "        # save data descriptors\n",
    "        self.tokenizer = tokenizer  # how to tokenize data\n",
    "        self.train = train  # whether it is train data\n",
    "        self.label2id = dict() if label2idx is None else label2idx  # dict is mutable\n",
    "\n",
    "        # align tokenized text & entities\n",
    "        self.tokenized_texts = []\n",
    "        self.aligned_entities = []\n",
    "\n",
    "        for idx in tqdm(range(raw_df['text'].shape[0])):\n",
    "            # get raw text & entities\n",
    "            ents = raw_df.entities.iloc[idx] if self.train else []\n",
    "            sentence = raw_df.text.iloc[idx]\n",
    "\n",
    "            # word tokenization\n",
    "            new_sent, new_ents = self.get_word_tokens_ents(sentence, ents)\n",
    "\n",
    "            # usual tokenization\n",
    "            inputs = self.tokenizer(\n",
    "                new_sent,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                is_split_into_words=True\n",
    "            )\n",
    "\n",
    "            # align labels with tokens\n",
    "            aligned_ents = []\n",
    "            if self.train:\n",
    "                aligned_ents = self.align_labels_with_tokens(new_ents, inputs.word_ids())\n",
    "\n",
    "            # save preprocessed data\n",
    "            self.tokenized_texts.append(inputs)\n",
    "            self.aligned_entities.append(aligned_ents)\n",
    "\n",
    "        # compute length of dataset\n",
    "        self.length = len(self.aligned_entities)\n",
    "\n",
    "    def get_word_tokens_ents(self, sentence: str, ents: list[str]) -> tuple[list[str], list[list[str]]]:\n",
    "        \"\"\"\n",
    "        Word tokenization for sentence with alignment of entities\n",
    "        \n",
    "        :param sentence:  sentence to split to words\n",
    "        :param ents:      raw entities in given sentence\n",
    "        :return:          (word tokenized sentence, aligned entities)\n",
    "        \"\"\"\n",
    "        \n",
    "        # ent index to post-tokenized index\n",
    "        idx_to_post_process_idx = dict()\n",
    "\n",
    "        # new word tokens (basically split by ' \\t\\n')\n",
    "        word_tokens = ['']\n",
    "        len_tokens = 0\n",
    "        for idx, symb in enumerate(sentence):\n",
    "            if symb in {' ', '\\t', '\\n'}:\n",
    "\n",
    "                # if new word is not already created\n",
    "                if word_tokens and word_tokens[-1] != '':\n",
    "                    word_tokens.append('')\n",
    "                    len_tokens += 1\n",
    "                continue\n",
    "\n",
    "            # add symbol to last word\n",
    "            word_tokens[-1] += symb\n",
    "            # map symbol index to new token index\n",
    "            idx_to_post_process_idx[idx] = len_tokens\n",
    "\n",
    "        # remove empty symbol from the back (if needed)\n",
    "        if word_tokens[-1] == '':\n",
    "            word_tokens.pop()\n",
    "\n",
    "        # new entetis\n",
    "        new_ents = [[] for _ in range(len(word_tokens))]\n",
    "\n",
    "        for ent in ents:\n",
    "            # parse raw ents\n",
    "            start_idx, end_idx, ent_name = ent.split()\n",
    "            start_idx, end_idx = int(start_idx), int(end_idx)\n",
    "            ent_name = self.label2id.get(ent_name, ent_name)\n",
    "\n",
    "            # control number of adds\n",
    "            placed_idxs = dict()\n",
    "\n",
    "            for idx in range(start_idx, end_idx):\n",
    "                # transform symb index to token index\n",
    "                new_idx = idx_to_post_process_idx.get(idx)\n",
    "\n",
    "                # if present & not already placed, place it\n",
    "                if (new_idx is not None) and (not placed_idxs.get(new_idx, False)):\n",
    "                    new_ents[new_idx].append(ent_name)\n",
    "                    placed_idxs[new_idx] = True\n",
    "\n",
    "        return word_tokens, new_ents\n",
    "\n",
    "    @staticmethod\n",
    "    def align_labels_with_tokens(labels: list[list[str]], word_ids: list[int]) -> list[list[str]]:\n",
    "        \"\"\"\n",
    "        Align entities labels with post-tokenized words\n",
    "        \n",
    "        :param labels:    entities from word tokenization\n",
    "        :param word_ids:  word id's from `self.tokenizer`\n",
    "        :return:          entities aligned to `word_ids` \n",
    "        \"\"\"\n",
    "        \n",
    "        # aligned labels\n",
    "        new_labels = []\n",
    "        current_word = None\n",
    "\n",
    "        for word_id in word_ids:\n",
    "            if word_id != current_word:\n",
    "                # start of a new word => add new label\n",
    "                current_word = word_id\n",
    "                label = [] if word_id is None else labels[word_id]\n",
    "                new_labels.append(label)\n",
    "            elif word_id is None:\n",
    "                # Special token => add empty label\n",
    "                new_labels.append([])\n",
    "            else:\n",
    "                # Same word as previous token => copy label\n",
    "                label = labels[word_id]\n",
    "                new_labels.append(label)\n",
    "\n",
    "        return new_labels\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.tokenized_texts[idx], self.aligned_entities[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28edc408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 519/519 [00:01<00:00, 375.81it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 93/93 [00:00<00:00, 359.04it/s]\n"
     ]
    }
   ],
   "source": [
    "def extract_ent_labels(raw_df: pd.DataFrame) -> set[str]:\n",
    "    \"\"\"\n",
    "    Extract entities names from dataframe\n",
    "    \n",
    "    :param raw_df:  dataframe with `entities` column\n",
    "    :return:        set of entities names\n",
    "    \"\"\"\n",
    "    \n",
    "    ent_names = set()\n",
    "    for ents in raw_df['entities']:\n",
    "        for ent in ents:\n",
    "            ent_names.add(ent.split()[-1])\n",
    "    return ent_names\n",
    "\n",
    "\n",
    "# entities encoding/decoding\n",
    "label2idx = dict()\n",
    "idx2label = dict()\n",
    "\n",
    "# for each entity make an index\n",
    "for idx, label in enumerate(sorted(extract_ent_labels(train))):\n",
    "    label2idx[label] = idx\n",
    "    idx2label[idx] = label\n",
    "\n",
    "\n",
    "# define train / eval dataloaders\n",
    "train_dataloader = NestedNerDataset(train.copy(), tokenizer=tokenizer, label2idx=label2idx)\n",
    "eval_dataloader = NestedNerDataset(eval_df.copy(), tokenizer=tokenizer, label2idx=label2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffb7b48",
   "metadata": {},
   "source": [
    "### Model architecture\n",
    "\n",
    "Now when we have data preprocessed let's discuss the architecture.\n",
    "\n",
    "It is obvious that we will use bert (as we already preprocessed data for them), but how? The idea I want to implement is - get token embiddings with bert-like models, and then train classifiers for token classification. As we have 29 classes of data, and we want \"nested\" dependencies, we will train 29 catboost classifiers (one for each entity type). \n",
    "\n",
    "This architecture is simmilar to one developed by [SinaLab](https://github.com/SinaLab/ArabicNER) for Arabic. \n",
    "\n",
    "The main difference is that they had several classes and each class had different types (Like entity \"Human\" could be \"Name\", \"Surname\", \"Age\" e.t.c). So my solution is simplification of their architecture. However my solution could be simply rewritten if required. \n",
    "\n",
    "Also they trained their own bert model, however I am interested how raw bert (not even fine-tuned) would perform in such task. So in implementation below I would just use bert for data preprocessing.\n",
    "\n",
    "Now let's discuss classifiers a bit. In SinaLab they used two-layer NN (786->hid->1), but now I will use Catboosts as different approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0a77d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 519/519 [00:12<00:00, 42.36it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_bert_vectors_labels(dataloader: NestedNerDataset, model):\n",
    "    \"\"\"\n",
    "    Transforms dataset samples to BERT vectors\n",
    "    \n",
    "    :param dataloader:  NestedNerDataset class instance\n",
    "    :param model:       bert-model class instance\n",
    "    :return:            vectors of tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # output containers\n",
    "    combined_ids = []\n",
    "    combined_preds = []\n",
    "    combined_labels = []\n",
    "    \n",
    "    # for each data sample get bert embeddings & save them\n",
    "    for data in tqdm(dataloader, total=len(dataloader)):\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs = inputs.to(model.device)\n",
    "        \n",
    "        preds = model(**inputs)['last_hidden_state'].squeeze().to('cpu')\n",
    "        \n",
    "        # save results\n",
    "        combined_preds.extend(preds.tolist())\n",
    "        combined_labels.extend(labels)\n",
    "        combined_ids.extend(inputs['input_ids'].tolist()[0])\n",
    "    \n",
    "    return combined_ids, combined_preds, combined_labels\n",
    "\n",
    "\n",
    "combined_ids, combined_preds, combined_labels = get_bert_vectors_labels(train_dataloader, bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81620640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147100, 147100, 147100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_ids), len(combined_preds), len(combined_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5347f076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # free gpu & models from bert (takes massive part of my GPU :3)\n",
    "# bert_model.to('cpu')\n",
    "# None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c13e78ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>y_20</th>\n",
       "      <th>y_21</th>\n",
       "      <th>y_22</th>\n",
       "      <th>y_23</th>\n",
       "      <th>y_24</th>\n",
       "      <th>y_25</th>\n",
       "      <th>y_26</th>\n",
       "      <th>y_27</th>\n",
       "      <th>y_28</th>\n",
       "      <th>ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.275032</td>\n",
       "      <td>0.131663</td>\n",
       "      <td>-0.064355</td>\n",
       "      <td>-0.474492</td>\n",
       "      <td>0.531660</td>\n",
       "      <td>0.138446</td>\n",
       "      <td>-0.288945</td>\n",
       "      <td>0.198151</td>\n",
       "      <td>0.009812</td>\n",
       "      <td>0.098127</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.356811</td>\n",
       "      <td>-0.303299</td>\n",
       "      <td>0.140634</td>\n",
       "      <td>-0.851324</td>\n",
       "      <td>0.468359</td>\n",
       "      <td>0.701960</td>\n",
       "      <td>-0.557334</td>\n",
       "      <td>0.238785</td>\n",
       "      <td>0.365653</td>\n",
       "      <td>-0.379581</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.110960</td>\n",
       "      <td>0.517242</td>\n",
       "      <td>-1.426707</td>\n",
       "      <td>-0.313037</td>\n",
       "      <td>0.358637</td>\n",
       "      <td>0.459469</td>\n",
       "      <td>-0.035206</td>\n",
       "      <td>0.565469</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.471379</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.433637</td>\n",
       "      <td>-0.572710</td>\n",
       "      <td>-0.172019</td>\n",
       "      <td>-0.589095</td>\n",
       "      <td>-0.093720</td>\n",
       "      <td>1.895635</td>\n",
       "      <td>-0.060754</td>\n",
       "      <td>0.039899</td>\n",
       "      <td>0.228753</td>\n",
       "      <td>-1.412677</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>82820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.433975</td>\n",
       "      <td>-0.431674</td>\n",
       "      <td>-1.391628</td>\n",
       "      <td>-0.638494</td>\n",
       "      <td>0.111096</td>\n",
       "      <td>0.128717</td>\n",
       "      <td>-1.348709</td>\n",
       "      <td>0.649881</td>\n",
       "      <td>0.045734</td>\n",
       "      <td>-1.525975</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 799 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.275032  0.131663 -0.064355 -0.474492  0.531660  0.138446 -0.288945   \n",
       "1 -0.356811 -0.303299  0.140634 -0.851324  0.468359  0.701960 -0.557334   \n",
       "2 -1.110960  0.517242 -1.426707 -0.313037  0.358637  0.459469 -0.035206   \n",
       "3  0.433637 -0.572710 -0.172019 -0.589095 -0.093720  1.895635 -0.060754   \n",
       "4 -0.433975 -0.431674 -1.391628 -0.638494  0.111096  0.128717 -1.348709   \n",
       "\n",
       "          7         8         9  ...  y_20  y_21  y_22  y_23  y_24  y_25  \\\n",
       "0  0.198151  0.009812  0.098127  ...     0     0     0     0     0     0   \n",
       "1  0.238785  0.365653 -0.379581  ...     0     0     0     0     0     0   \n",
       "2  0.565469  0.536585  0.471379  ...     0     0     0     0     0     0   \n",
       "3  0.039899  0.228753 -1.412677  ...     0     0     1     0     0     0   \n",
       "4  0.649881  0.045734 -1.525975  ...     0     0     0     0     0     0   \n",
       "\n",
       "   y_26  y_27  y_28    ids  \n",
       "0     0     0     0    101  \n",
       "1     0     0     0  37104  \n",
       "2     0     0     0  65193  \n",
       "3     0     0     0  82820  \n",
       "4     0     0     0    851  \n",
       "\n",
       "[5 rows x 799 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make custom dataframe\n",
    "train_df = pd.DataFrame(combined_preds)\n",
    "\n",
    "# original y (list of labels)\n",
    "train_df['y'] = combined_labels\n",
    "\n",
    "# for each y value make one-hot-encoded column\n",
    "for class_i in range(len(label2idx)):\n",
    "    mask = train_df['y'].apply(lambda ls: class_i in ls)\n",
    "    train_df[f'y_{class_i}'] = mask.astype(int)\n",
    "\n",
    "# save original ids of tokens\n",
    "train_df['ids'] = combined_ids\n",
    "\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dfb72dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/29 [00:00<?, ?it/s]Warning: less than 75% gpu memory available for training. Free: 5253 Total: 8191.6875\n",
      "  3%|██▊                                                                                | 1/29 [00:21<09:51, 21.11s/it]Warning: less than 75% gpu memory available for training. Free: 5253 Total: 8191.6875\n",
      "  7%|█████▋                                                                             | 2/29 [00:42<09:30, 21.13s/it]Warning: less than 75% gpu memory available for training. Free: 5253 Total: 8191.6875\n",
      " 10%|████████▌                                                                          | 3/29 [01:03<09:14, 21.33s/it]Warning: less than 75% gpu memory available for training. Free: 5253 Total: 8191.6875\n",
      " 14%|███████████▍                                                                       | 4/29 [01:26<09:09, 21.98s/it]Warning: less than 75% gpu memory available for training. Free: 5253 Total: 8191.6875\n",
      " 17%|██████████████▎                                                                    | 5/29 [01:51<09:10, 22.92s/it]Warning: less than 75% gpu memory available for training. Free: 5253 Total: 8191.6875\n",
      " 21%|█████████████████▏                                                                 | 6/29 [02:17<09:14, 24.09s/it]Warning: less than 75% gpu memory available for training. Free: 5253 Total: 8191.6875\n",
      " 24%|████████████████████                                                               | 7/29 [02:42<08:52, 24.22s/it]Warning: less than 75% gpu memory available for training. Free: 5253 Total: 8191.6875\n",
      " 28%|██████████████████████▉                                                            | 8/29 [03:05<08:24, 24.01s/it]Warning: less than 75% gpu memory available for training. Free: 5253 Total: 8191.6875\n",
      " 31%|█████████████████████████▊                                                         | 9/29 [03:33<08:25, 25.29s/it]Warning: less than 75% gpu memory available for training. Free: 5253 Total: 8191.6875\n",
      " 34%|████████████████████████████▎                                                     | 10/29 [03:57<07:52, 24.86s/it]Warning: less than 75% gpu memory available for training. Free: 5253 Total: 8191.6875\n",
      " 38%|███████████████████████████████                                                   | 11/29 [04:19<07:08, 23.79s/it]Warning: less than 75% gpu memory available for training. Free: 5253 Total: 8191.6875\n",
      " 41%|█████████████████████████████████▉                                                | 12/29 [04:41<06:38, 23.44s/it]Warning: less than 75% gpu memory available for training. Free: 5253 Total: 8191.6875\n",
      " 45%|████████████████████████████████████▊                                             | 13/29 [05:04<06:09, 23.07s/it]Warning: less than 75% gpu memory available for training. Free: 5253 Total: 8191.6875\n",
      " 48%|███████████████████████████████████████▌                                          | 14/29 [05:30<06:00, 24.04s/it]Warning: less than 75% gpu memory available for training. Free: 5253 Total: 8191.6875\n",
      " 52%|██████████████████████████████████████████▍                                       | 15/29 [05:59<05:59, 25.71s/it]Warning: less than 75% gpu memory available for training. Free: 5253 Total: 8191.6875\n",
      " 55%|█████████████████████████████████████████████▏                                    | 16/29 [06:25<05:32, 25.55s/it]Warning: less than 75% gpu memory available for training. Free: 5253 Total: 8191.6875\n",
      " 59%|████████████████████████████████████████████████                                  | 17/29 [06:51<05:08, 25.70s/it]Warning: less than 75% gpu memory available for training. Free: 5253 Total: 8191.6875\n",
      " 62%|██████████████████████████████████████████████████▉                               | 18/29 [07:16<04:41, 25.57s/it]Warning: less than 75% gpu memory available for training. Free: 5253 Total: 8191.6875\n",
      " 66%|█████████████████████████████████████████████████████▋                            | 19/29 [07:41<04:13, 25.35s/it]Warning: less than 75% gpu memory available for training. Free: 5253 Total: 8191.6875\n",
      " 69%|████████████████████████████████████████████████████████▌                         | 20/29 [08:09<03:56, 26.30s/it]Warning: less than 75% gpu memory available for training. Free: 5253 Total: 8191.6875\n",
      " 72%|███████████████████████████████████████████████████████████▍                      | 21/29 [08:33<03:23, 25.41s/it]Warning: less than 75% gpu memory available for training. Free: 5253 Total: 8191.6875\n",
      " 76%|██████████████████████████████████████████████████████████████▏                   | 22/29 [08:57<02:54, 24.98s/it]Warning: less than 75% gpu memory available for training. Free: 5253 Total: 8191.6875\n",
      " 79%|█████████████████████████████████████████████████████████████████                 | 23/29 [09:25<02:36, 26.02s/it]Warning: less than 75% gpu memory available for training. Free: 5253 Total: 8191.6875\n",
      " 83%|███████████████████████████████████████████████████████████████████▊              | 24/29 [09:50<02:08, 25.61s/it]Warning: less than 75% gpu memory available for training. Free: 5253 Total: 8191.6875\n",
      " 86%|██████████████████████████████████████████████████████████████████████▋           | 25/29 [10:19<01:47, 26.87s/it]Warning: less than 75% gpu memory available for training. Free: 5253 Total: 8191.6875\n",
      " 90%|█████████████████████████████████████████████████████████████████████████▌        | 26/29 [10:43<01:17, 25.87s/it]Warning: less than 75% gpu memory available for training. Free: 5253 Total: 8191.6875\n",
      " 93%|████████████████████████████████████████████████████████████████████████████▎     | 27/29 [11:07<00:50, 25.34s/it]Warning: less than 75% gpu memory available for training. Free: 5253 Total: 8191.6875\n",
      " 97%|███████████████████████████████████████████████████████████████████████████████▏  | 28/29 [11:31<00:24, 24.82s/it]Warning: less than 75% gpu memory available for training. Free: 5253 Total: 8191.6875\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 29/29 [11:56<00:00, 24.70s/it]\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "\n",
    "# define all cats\n",
    "cats = [\n",
    "    CatBoostClassifier(iterations=1000, task_type=\"GPU\", devices='0')\n",
    "    for _ in range(len(label2idx))\n",
    "]\n",
    "\n",
    "\n",
    "emb_columns = [column for column in train_df.columns if type(column) == int]\n",
    "X = train_df[emb_columns]\n",
    "for class_i in tqdm(range(len(label2idx))):\n",
    "    cats[class_i].fit(X, train_df[f'y_{class_i}'], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73257ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # same as catboost , but for KNN and RandomForest, deprecated due to speed problems :3\n",
    "\n",
    "\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# knn_cats = [\n",
    "# #     RandomForestClassifier(n_estimators=10)\n",
    "#     KNeighborsClassifier(n_neighbors=5, metric='cosine', algorithm='brute')\n",
    "#     for _ in range(len(label2idx))\n",
    "# ]\n",
    "\n",
    "\n",
    "# emb_columns = [column for column in train_df.columns if type(column) == int]\n",
    "# X = train_df[emb_columns]\n",
    "# for class_i in tqdm(range(len(label2idx))):\n",
    "#     knn_cats[class_i].fit(X, train_df[f'y_{class_i}'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10917561",
   "metadata": {},
   "source": [
    "### From Predictions -> Human-Readable\n",
    "\n",
    "And finally when we have predictions, it make sence to discuss how wo transform raw token prediction back to slices & labels format. \n",
    "\n",
    "This was one of the most confusing steps for me, as we are not working with full words but subwords tokenization. \n",
    "\n",
    "The first thing is data we are getting is just a list of lists where each `ans[i]` correspond to `i`'th token, and `ans[i]` contains indexes of classes given token corresponds to. With this information we could potentially get tokens start/end indexes in original sentence. This is what is implemented in `map_results_to_ids` function. Basically it takes `token_id`, via tokenizer get the string representation and finds string map in the original sentence. This implementation is pretty nice as it do not cares about word tokenization (we don't need to consider skipped spaces/tabs/new-lines e.t.c.). Algorithm is modified a bit for `DeepPavlov/rubert-base-cased` as it strips special tokens that start with `#`. I tested this method on train/eval data and it resulted in perfect map. \n",
    "\n",
    "The second step is to combine continiouts entities. As an input we have (start, end, entities_list) and we want to combine tokens that we consider `continious` (e.g. `end[i] == start[i]`). For this we can use scanline algorithm implemented in `from_sequential_to_readable` function. Worth mentioning that we can define `error` of combination, where tokens on disstance less than `error` could be considered as one, this adds variability to result where results and could boost performance if error is tuned as needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c396bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(texts: list[str], cats: list[CatBoostClassifier], idx2label: dict, tokenizer, print_res: bool = False):\n",
    "    \"\"\"\n",
    "    Get model predictions for given texts\n",
    "    \n",
    "    :param texts:      texts for inference\n",
    "    :param cats:       list of catboost classifiers\n",
    "    :param idx2label:  mapping from index to entity label\n",
    "    :param tokenizer:  tokenizer used to tokenize texts\n",
    "    :param print_res:  whether to print predictions to console\n",
    "    :return:           model predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    # transform list data to NestedNerDataset\n",
    "    inf_df = pd.DataFrame()\n",
    "    inf_df['text'] = texts\n",
    "    inference_loader = NestedNerDataset(\n",
    "        inf_df,\n",
    "        tokenizer=tokenizer,\n",
    "        label2idx=label2idx,\n",
    "        train=False\n",
    "    )\n",
    "\n",
    "    # get bert vectors\n",
    "    combined_ids, combined_preds, combined_labels = get_bert_vectors_labels(inference_loader, bert_model)\n",
    "\n",
    "    # make custom dataframe\n",
    "    combined_df = pd.DataFrame(combined_preds)\n",
    "    emb_columns = [column for column in train_df.columns if type(column) == int]\n",
    "    combined_df['ids'] = combined_ids\n",
    "\n",
    "    # get predictions for each data sample\n",
    "    results = []\n",
    "    for _, row in tqdm(combined_df.iterrows(), total=combined_df.shape[0]):\n",
    "        results.append([])\n",
    "\n",
    "        row_logits = []\n",
    "        for class_id in range(len(idx2label)):\n",
    "            # for each class get predictions\n",
    "            row_df = row.to_frame().transpose()[emb_columns]\n",
    "            row_logits.append(cats[class_id].predict(row_df))\n",
    "\n",
    "        # determine indexes of positive classes\n",
    "        row_logits = torch.tensor(row_logits)\n",
    "        ners = (row_logits > 0.5).nonzero(as_tuple=True)[0].tolist()\n",
    "\n",
    "        # transform indexes to entities labels\n",
    "        ner_tags = [idx2label[ner_idx] for ner_idx in ners]\n",
    "\n",
    "        # get token string representation\n",
    "        token_str = tokenizer.decode(int(row['ids']))\n",
    "\n",
    "        # save token_id, token_str and ner labels\n",
    "        results[-1].append((\n",
    "            int(row['ids']),\n",
    "            token_str,\n",
    "            ner_tags\n",
    "        ))\n",
    "\n",
    "        if print_res:\n",
    "            print(token_str, '\\t:\\t', ' + '.join(ner_tags))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def map_results_to_ids(origs: list[str], results: list[list[tuple[int, str, list[str]]]]) -> list[(int, int, list[str])]:\n",
    "    \"\"\"\n",
    "    Get tokens start_idx, end_idx from the original text\n",
    "\n",
    "    :param origs:    original pre-tokenized texts\n",
    "    :param results:  raw predictions of model\n",
    "    :return:         tuples with indexes of tokens in original text\n",
    "    \"\"\"\n",
    "\n",
    "    answers = []\n",
    "    for orig, preds in zip(origs, results):\n",
    "        orig_ind = 0\n",
    "        ans = []\n",
    "\n",
    "        # remove <BOS> <EOS>\n",
    "        for token_id, str_token, labels in preds[1:-1]:\n",
    "            if str_token != '#':  # DeepPavlov/rubert-base-cased specifics\n",
    "                str_token = str_token.lstrip('#')\n",
    "            \n",
    "            # find start index of str_token in original text\n",
    "            find_idx = orig[orig_ind:].find(str_token)\n",
    "            \n",
    "            # (if token was <UNK>)\n",
    "            if find_idx == -1:\n",
    "                continue\n",
    "            \n",
    "            # update indexes \n",
    "            start_ind = orig_ind + find_idx\n",
    "            end_ind = start_ind + len(str_token)\n",
    "            orig_ind = end_ind\n",
    "            \n",
    "            # always true, but just in case smth is wrong\n",
    "            if str_token == orig[start_ind:end_ind]:  \n",
    "                ans.append((start_ind, end_ind, labels))\n",
    "            \n",
    "        answers.append(ans)\n",
    "    return answers\n",
    "\n",
    "\n",
    "def from_sequential_to_readable(\n",
    "        sequential_results: list[list[int, int, list[int]]],\n",
    "        error: int = 0\n",
    ") -> list[tuple[int, int, str]]:\n",
    "    \"\"\"\n",
    "    Combine continuous tokens into one with some error\n",
    "\n",
    "    :param sequential_results:  outputs of `map_results_to_ids`\n",
    "    :param error:               extra  between tokens to consider 'same'\n",
    "    :return:                    combined tokens\n",
    "    \"\"\"\n",
    "\n",
    "    answers = []\n",
    "    for sequence in sequential_results:\n",
    "        points = []\n",
    "        possible_labels = set()\n",
    "        \n",
    "        # split several labels to several data points\n",
    "        # example (0, 10, [PERSON, DATE] -> [(0, 10, PERSON), (0, 10, DATE)])\n",
    "        for start_idx, end_idx, labels in sequence:\n",
    "            for label in labels:\n",
    "                points.append((start_idx, end_idx, label))\n",
    "                possible_labels.add(label)\n",
    "        \n",
    "        # to remove data cleaning further\n",
    "        for label in possible_labels:\n",
    "            points.append((int(1e9), int(1e9), label))\n",
    "        \n",
    "        ans = []\n",
    "        prev_encounters = dict()\n",
    "        # point already sorted, so now we can implement scanline\n",
    "        for start_idx, end_idx, label in points:\n",
    "            # if too far from previous token, make new one\n",
    "            if prev_encounters.get(label, [-int(1e9), -int(1e9)])[1] + error < start_idx:\n",
    "                # if not first, add to answer\n",
    "                if prev_encounters.get(label) is not None:\n",
    "                    ans.append([*prev_encounters[label], label])\n",
    "                \n",
    "                prev_encounters[label] = [start_idx, end_idx]\n",
    "            # if not too far, change end_idx\n",
    "            else:  \n",
    "                prev_encounters[label][1] = end_idx\n",
    "        answers.append(tuple(ans))\n",
    "\n",
    "    return answers\n",
    "\n",
    "\n",
    "def get_answers(texts: list[str], cats: list[CatBoostClassifier], idx2label: dict[int, str], tokenizer) -> list[tuple[int, int, str]]:\n",
    "    \"\"\"\n",
    "    Full pipeline that runs `map_results_to_ids` -> `mapped` -> `from_sequential_to_readable` sequentially\n",
    "\n",
    "    :param texts:      texts for inference\n",
    "    :param cats:       list of catboost classifiers\n",
    "    :param idx2label:  mapping from index to entity label\n",
    "    :param tokenizer:  tokenizer used to tokenize texts\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    raw_predictions = inference(texts, cats, idx2label, tokenizer, print_res=False)\n",
    "    mapped = map_results_to_ids(texts, raw_predictions)\n",
    "    sequential = from_sequential_to_readable(mapped, error=2)\n",
    "    return sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae21608",
   "metadata": {},
   "source": [
    "### Metric calculation \n",
    "\n",
    "Even though model will be measured by f1-macro, I wanted to see how model performs in general (something like accuracy of some sort). As we are working with sets (both ranges and class type), the simplest thing would be to consider set's IOU. This metric is not interpretable and do not shows insides where model makes mistakes, but it is nice to track how model performs \"in general\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ea434c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_iou(texts, labels, cats, idx2label, tokenizer):\n",
    "    ious = []\n",
    "    for model_prediction, target_label in zip(get_answers(texts, cats, idx2label, tokenizer), labels):\n",
    "        # transform to original format\n",
    "        model_prediction = [f\"{s} {e} {t}\" for s, e, t in model_prediction]\n",
    "        \n",
    "        iou = round(100*len(set(model_prediction) & set(target_label)) / len(set(model_prediction) | set(target_label)), 3)\n",
    "        ious.append(iou)\n",
    "    return np.array(ious)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ece35af",
   "metadata": {},
   "source": [
    "### Train, Evaluate and Inference\n",
    "\n",
    "Basically combination of everything above to test solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59444911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 333.25it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  8.44it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 584/584 [17:19<00:00,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "iou = get_iou(eval_df.text.iloc[:2].tolist(), eval_df.entities.iloc[:2].tolist(), cats, idx2label, tokenizer)\n",
    "print(iou.mean(), iou.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "245a025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # make predictions on test data\n",
    "# input_jsons = []\n",
    "# with open('../../data/test.jsonl', 'r', encoding='UTF-8') as file:\n",
    "#     for line in file.readlines():\n",
    "#         input_jsons.append(json.loads(line))\n",
    "\n",
    "# with open('test.jsonl', 'w') as file:\n",
    "#     for test_json in input_jsons:\n",
    "#         ans_json = {\n",
    "#             'id': test_json['id'],\n",
    "#             'ners': get_answers([test_json['senences']], cats, idx2label, tokenizer)[0]\n",
    "#         }\n",
    "        \n",
    "#         file.write(json.dumps(ans_json) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c379a6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
